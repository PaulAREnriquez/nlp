{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### resource: https://github.com/Applied-Language-Technology/notebooks/blob/main/part_iii/05_embeddings_continued.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **E. Contextual Embeddings**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the embeddings that we have learned are described as `static`. This means that the vector representation for a given token **regardless** of the `context` in which the token occurs.\n",
    "\n",
    "Words can have more than one meaning depending on the context they appear:\n",
    "\n",
    "1. Manila is the `capital` of the Philippines.\n",
    "2. The most important advice anyone can receive in trading the market is risk management - protect your `capital`, profits just follow.\n",
    "\n",
    "Static word embeddings cannot capture this distinction because words will always have the same vector representation. The values of the `n-dimensional` vector remains the same.\n",
    "\n",
    "We would like to encode the context by which a word occurs to draw out these differences in meaning, using an architecture called **`Transformers`**.\n",
    "\n",
    "A **`Transformer`** is a neural network architecture that is capable of encoding information about the context which a token occurs. It improved performance for traditional natural language processing tasks such as part-of-speech tagging and syntactic parsing, but it is `computationally heavy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Transformer-based language model\n",
    "nlp_trf = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the surface, a *Language* object that contains a Transformer-based model looks and works just like any other language model in spaCy.\n",
    "\n",
    "However, if we look under the hood of the *Language* object under `nlp_trf` using `pipeline`, we will see that the first component in the processing pipeline is a Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x1fe0b8c4b20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1fe0b8c4f40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1fe0b71bf20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1fe0b90fa80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1fe0b911840>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1fe0b8ff120>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_trf.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feed an example sentence to the model\n",
    "example_doc = nlp_trf('The most important advice anyone can receive in trading the market is risk management - protect your capital, profits just follow ~ Paul Aldrin Enriquez')\n",
    "\n",
    "# Check the length of the document\n",
    "# It counts the punctuation marks\n",
    "len(example_doc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the transformer is stored in an object in spaCy called `trf_data`.\n",
    "\n",
    "The `tensors` attribute of the *TransformerData* object contains a list with vector representations generated by the Transformer for individual *Tokens* and the entire *Doc* object.\n",
    "\n",
    "The first item in the `tensors` list under index 0 contains the output for the individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 31, 768)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc._.trf_data.tensors[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there is one batch of `31` vectors, each with `768` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3221337 ,  0.03971476, -1.6743078 , ...,  0.31791827,\n",
       "         0.4955263 , -0.5075195 ],\n",
       "       [-0.6849789 , -0.2843948 ,  0.740689  , ..., -1.0440093 ,\n",
       "         0.07250463, -0.9174782 ],\n",
       "       [ 0.9529375 ,  0.42180288, -0.4646021 , ..., -0.01987644,\n",
       "        -0.69030166,  1.3408207 ],\n",
       "       ...,\n",
       "       [ 0.59506214, -1.7604082 ,  0.11381152, ..., -2.634387  ,\n",
       "         0.59920263,  0.68672407],\n",
       "       [-0.6051442 , -0.01173162,  0.52655953, ...,  0.20555082,\n",
       "         0.08978168, -1.4721365 ],\n",
       "       [ 0.39855224, -1.3170516 , -0.554474  , ..., -1.1178719 ,\n",
       "        -0.03808063,  0.48295164]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first ten dimensions of the tensor\n",
    "example_doc._.trf_data.tensors[0][0][:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second item under index 1 holds the output for the entire *Doc* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc._.trf_data.tensors[1].shape\n",
    "# A single vector with 768 dimensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformers object output 24 vectors for a spaCy Doc object of 22 tokens. They don't match.\n",
    "\n",
    "Vocabulary size of all words just in the English language alone is massive, and to learn every unique word would blow up the size of the model.\n",
    "\n",
    "Because Transformers are trained on massive volumes of text, the model's vocabulary must be kept limited.\n",
    "\n",
    "To address the issue, Transformers use more complex tokenizers that identify frequently occuring character sequences in the data and learn embeddings for these sequences instead. These sequences, which are often referred as `subwords`, make up the vocabulary of the Transformer.\n",
    "\n",
    "This information is stored in the attribute `tokens`, which contains a dictionary of subwords under the key `input_texts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>',\n",
       "  'The',\n",
       "  'Ġmost',\n",
       "  'Ġimportant',\n",
       "  'Ġadvice',\n",
       "  'Ġanyone',\n",
       "  'Ġcan',\n",
       "  'Ġreceive',\n",
       "  'Ġin',\n",
       "  'Ġtrading',\n",
       "  'Ġthe',\n",
       "  'Ġmarket',\n",
       "  'Ġis',\n",
       "  'Ġrisk',\n",
       "  'Ġmanagement',\n",
       "  'Ġ-',\n",
       "  'Ġprotect',\n",
       "  'Ġyour',\n",
       "  'Ġcapital',\n",
       "  ',',\n",
       "  'Ġprofits',\n",
       "  'Ġjust',\n",
       "  'Ġfollow',\n",
       "  'Ġ~',\n",
       "  'ĠPaul',\n",
       "  'ĠAld',\n",
       "  'rin',\n",
       "  'ĠEn',\n",
       "  'ri',\n",
       "  'quez',\n",
       "  '</s>']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_doc._.trf_data.tokens['input_texts']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see how the document has been tokenized. They begin and terminate with the tag `<s>`, and the `Ġ` indicates space. The more interesting part is how my name has been tokenized. \n",
    "`'ĠPaul',`\n",
    "  `'ĠAld',`\n",
    "  `'rin',`\n",
    "  `'ĠEn',`\n",
    "  `'ri',`\n",
    "  `'quez'`\n",
    "\n",
    "This is so because my name is not in the Transformer vocabulary, but the subwords of my name are. Their individual vector representations will be used to construct the vector for each word of my name.\n",
    "\n",
    "This is done for optimization. The subwords that make up the vocabulary are those that occur frequently in the training data the Transformer is trained with."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To map these vectors to `Tokens` in the spaCy Doc object, we must retrieve alignment information from the `align` attribute of the TransformerData object.\n",
    "\n",
    "The `align` attribute can be indexed using the indices of Token objects in the Doc object.\n",
    "\n",
    "To exemplify, we can retrieve the last Token \"Enriquez\" in the Doc object doc using the expression `example_doc[0]`.\n",
    "\n",
    "We then use the index of this Token in the Doc object to retrieve alignment data, which is stored under the `align` attribute.\n",
    "\n",
    "More specifically, we need the information stored under the attribute `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Enriquez,\n",
       " array([[27],\n",
       "        [28],\n",
       "        [29]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get the last spaCy Token, \"Enriquez\", and its alignment data\n",
    "example_doc[-1], example_doc._.trf_data.align[-1].data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, vectors at indices 27, 28 and 29 in the batch of 31 vectors contain the representation for \"Enriquez\".\n",
    "\n",
    "To use the contextual embeddings from the Transformer efficiently, we can define a component that retrieves contextual word embeddings for Docs, Spans and Tokens and add this component to the spaCy pipeline.\n",
    "\n",
    "This can be achieved by creating a new Python Class. Because the new Class will become a component of the spaCy pipeline, we must first import the Language object and let spaCy know that we are now defining a new pipeline component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Language object under the 'language' module in spaCy,\n",
    "# and NumPy for calculating cosine similarity.\n",
    "from spacy.language import Language\n",
    "import numpy as np\n",
    "\n",
    "# We use the @ character to register the following Class definition\n",
    "# with spaCy under the name 'tensor2attr'.\n",
    "@Language.factory('tensor2attr')\n",
    "\n",
    "# We begin by declaring the class name: Tensor2Attr. The name is \n",
    "# declared using 'class', followed by the name and a colon.\n",
    "class Tensor2Attr:\n",
    "    \n",
    "    # We continue by defining the first method of the class, \n",
    "    # __init__(), which is called when this class is used for \n",
    "    # creating a Python object. Custom components in spaCy \n",
    "    # require passing two variables to the __init__() method:\n",
    "    # 'name' and 'nlp'. The variable 'self' refers to any\n",
    "    # object created using this class!\n",
    "    def __init__(self, name, nlp):\n",
    "        \n",
    "        # We do not really do anything with this class, so we\n",
    "        # simply move on using 'pass' when the object is created.\n",
    "        pass\n",
    "\n",
    "    # The __call__() method is called whenever some other object\n",
    "    # is passed to an object representing this class. Since we know\n",
    "    # that the class is a part of the spaCy pipeline, we already know\n",
    "    # that it will receive Doc objects from the preceding layers.\n",
    "    # We use the variable 'doc' to refer to any object received.\n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        # When an object is received, the class will instantly pass\n",
    "        # the object forward to the 'add_attributes' method. The\n",
    "        # reference to self informs Python that the method belongs\n",
    "        # to this class.\n",
    "        self.add_attributes(doc)\n",
    "        \n",
    "        # After the 'add_attributes' method finishes, the __call__\n",
    "        # method returns the object.\n",
    "        return doc\n",
    "    \n",
    "    # Next, we define the 'add_attributes' method that will modify\n",
    "    # the incoming Doc object by calling a series of methods.\n",
    "    def add_attributes(self, doc):\n",
    "        \n",
    "        # spaCy Doc objects have an attribute named 'user_hooks',\n",
    "        # which allows customising the default attributes of a \n",
    "        # Doc object, such as 'vector'. We use the 'user_hooks'\n",
    "        # attribute to replace the attribute 'vector' with the \n",
    "        # Transformer output, which is retrieved using the \n",
    "        # 'doc_tensor' method defined below.\n",
    "        doc.user_hooks['vector'] = self.doc_tensor\n",
    "        \n",
    "        # We then perform the same for both Spans and Tokens that\n",
    "        # are contained within the Doc object.\n",
    "        doc.user_span_hooks['vector'] = self.span_tensor\n",
    "        doc.user_token_hooks['vector'] = self.token_tensor\n",
    "        \n",
    "        # We also replace the 'similarity' method, because the \n",
    "        # default 'similarity' method looks at the default 'vector'\n",
    "        # attribute, which is empty! We must first replace the\n",
    "        # vectors using the 'user_hooks' attribute.\n",
    "        doc.user_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_span_hooks['similarity'] = self.get_similarity\n",
    "        doc.user_token_hooks['similarity'] = self.get_similarity\n",
    "    \n",
    "    # Define a method that takes a Doc object as input and returns \n",
    "    # Transformer output for the entire Doc.\n",
    "    def doc_tensor(self, doc):\n",
    "        \n",
    "        # Return Transformer output for the entire Doc. As noted\n",
    "        # above, this is the last item under the attribute 'tensor'.\n",
    "        # Average the output along axis 0 to handle batched outputs.\n",
    "        return doc._.trf_data.tensors[-1].mean(axis=0)\n",
    "    \n",
    "    # Define a method that takes a Span as input and returns the Transformer \n",
    "    # output.\n",
    "    def span_tensor(self, span):\n",
    "        \n",
    "        # Get alignment information for Span. This is achieved by using\n",
    "        # the 'doc' attribute of Span that refers to the Doc that contains\n",
    "        # this Span. We then use the 'start' and 'end' attributes of a Span\n",
    "        # to retrieve the alignment information. Finally, we flatten the\n",
    "        # resulting array to use it for indexing.\n",
    "        tensor_ix = span.doc._.trf_data.align[span.start: span.end].data.flatten()\n",
    "        \n",
    "        # Fetch Transformer output shape from the final dimension of the output.\n",
    "        # We do this here to maintain compatibility with different Transformers,\n",
    "        # which may output tensors of different shape.\n",
    "        out_dim = span.doc._.trf_data.tensors[0].shape[-1]\n",
    "        \n",
    "        # Get Token tensors under tensors[0]. Reshape batched outputs so that\n",
    "        # each \"row\" in the matrix corresponds to a single token. This is needed\n",
    "        # for matching alignment information under 'tensor_ix' to the Transformer\n",
    "        # output.\n",
    "        tensor = span.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "        \n",
    "        # Average vectors along axis 0 (\"columns\"). This yields a 768-dimensional\n",
    "        # vector for each spaCy Span.\n",
    "        return tensor.mean(axis=0)\n",
    "    \n",
    "    # Define a function that takes a Token as input and returns the Transformer\n",
    "    # output.\n",
    "    def token_tensor(self, token):\n",
    "        \n",
    "        # Get alignment information for Token; flatten array for indexing.\n",
    "        # Again, we use the 'doc' attribute of a Token to get the parent Doc,\n",
    "        # which contains the Transformer output.\n",
    "        tensor_ix = token.doc._.trf_data.align[token.i].data.flatten()\n",
    "        \n",
    "        # Fetch Transformer output shape from the final dimension of the output.\n",
    "        # We do this here to maintain compatibility with different Transformers,\n",
    "        # which may output tensors of different shape.\n",
    "        out_dim = token.doc._.trf_data.tensors[0].shape[-1]\n",
    "        \n",
    "        # Get Token tensors under tensors[0]. Reshape batched outputs so that\n",
    "        # each \"row\" in the matrix corresponds to a single token. This is needed\n",
    "        # for matching alignment information under 'tensor_ix' to the Transformer\n",
    "        # output.\n",
    "        tensor = token.doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n",
    "\n",
    "        # Average vectors along axis 0 (columns). This yields a 768-dimensional\n",
    "        # vector for each spaCy Token.\n",
    "        return tensor.mean(axis=0)\n",
    "    \n",
    "    # Define a function for calculating cosine similarity between vectors\n",
    "    def get_similarity(self, doc1, doc2):\n",
    "        \n",
    "        # Calculate and return cosine similarity\n",
    "        return np.dot(doc1.vector, doc2.vector) / (doc1.vector_norm * doc2.vector_norm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Class Tensor2Attr defined, we can now add it to the pipeline by referring to the name we registered with spaCy using `@Language.factory()`, that is, `tensor2attr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('transformer',\n",
       "  <spacy_transformers.pipeline_component.Transformer at 0x1fe0b8c4b20>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1fe0b8c4f40>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1fe0b71bf20>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1fe0b90fa80>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1fe0b911840>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1fe0b8ff120>),\n",
       " ('tensor2attr', <__main__.Tensor2Attr at 0x1fe0d8d0dc0>)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the component named 'tensor2attr', which we registered using the\n",
    "# @Language decorator and its 'factory' method to the pipeline.\n",
    "nlp_trf.add_pipe('tensor2attr')\n",
    "\n",
    "# Call the 'pipeline' attribute to examine the pipeline\n",
    "nlp_trf.pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component stores the Transformer-based contextual embeddings for Docs, Spans and Tokens under the vector attribute.\n",
    "\n",
    "Let's explore contextual embeddings by defining two Doc objects and feeding them to the Transformer-based language model under `nlp_trf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two example sentences and process them using the Transformer-based\n",
    "# language model under 'nlp_trf'.\n",
    "doc_city_trf = nlp_trf(\"Manila is the capital of the Philippines.\")\n",
    "doc_money_trf = nlp_trf(\"The most important advice anyone can receive in trading the market is risk management - protect your capital, profits just follow.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The noun `\"capital\"` has two different meanings in these sentences: in doc_city_trf, `\"capital\"` refers to a `city`, whereas in doc_money_trf the word refers to `money`.\n",
    "\n",
    "The Transformer should encode this difference into the resulting vector based on the context in which the word occurs.\n",
    "\n",
    "Let's fetch the Token corresponding to `\"capital\"` in each example and retrieve their vector representations under the `vector` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64822173"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve vectors for the two Tokens corresponding to \"capital\";\n",
    "# assign to variables 'city_trf' and 'money_trf'.\n",
    "city_trf = doc_city_trf[3]\n",
    "money_trf = doc_money_trf[17]\n",
    "\n",
    "# Compare the similarity of the two meanings of 'capital'\n",
    "city_trf.similarity(money_trf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity is `less than 1`, which means that their vector representations even they are the same word `capital` are different. This is because the Transformer also encodes information abouth their context of occurence into the vectors, which has allowed it to learn that the same linguistic form may have different meanings in different contexts.\n",
    "\n",
    "This stands in stark contrast to `static` word embeddings, as shown below using a large language model for English in spaCy `en_core_web_lg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define two example sentences and process them using the large language model\n",
    "# en_core_web_lg is a large language model for English, which contains word vectors for 685 000 Token objects.\n",
    "\n",
    "# Load a large language model and assign it to the variable 'nlp_lg'\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "doc_city_lg = nlp_lg(\"Manila is the capital of the Philippines.\")\n",
    "doc_money_lg = nlp_lg(\"The most important advice anyone can receive in trading the market is risk management - protect your capital, profits just follow.\")\n",
    "\n",
    "# Retrieve vectors for the two Tokens corresponding to \"capital\";\n",
    "# assign to variables 'city_lg' and 'money_lg'.\n",
    "city_lg = doc_city_lg[3]\n",
    "money_lg = doc_money_lg[17]\n",
    "\n",
    "# Compare the similarity of the two meanings of 'capital'\n",
    "city_lg.similarity(money_lg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the vectors for the word `\"capital\"` are identical `(cosine similarity of 1)`, because the word embeddings do not encode information about the context in which the word occurs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **End. Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcad393ebe0ddd96229d28636729608eefe2a539d1a2b16c2babaf1f7828873b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
