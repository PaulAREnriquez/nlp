{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arirang simply kpop kim hyung jun cross ha yeo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read politico article donald trump running mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type bazura project google image image photo d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fast lerner subpoena tech guy work hillary pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sony reward app like lot female singer non ret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0  arirang simply kpop kim hyung jun cross ha yeo...\n",
       "1  read politico article donald trump running mat...\n",
       "2  type bazura project google image image photo d...\n",
       "3  fast lerner subpoena tech guy work hillary pri...\n",
       "4  sony reward app like lot female singer non ret..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_tokenized = pd.read_csv('csvs/tweets_train_tokens.csv', index_col=False)\n",
    "tweets_train_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        arirang simply kpop kim hyung jun cross ha yeo...\n",
       "1        read politico article donald trump running mat...\n",
       "2        type bazura project google image image photo d...\n",
       "3        fast lerner subpoena tech guy work hillary pri...\n",
       "4        sony reward app like lot female singer non ret...\n",
       "                               ...                        \n",
       "49670    sleep think fuck jordan answer phone tomorrow ...\n",
       "49671    yoga shannon tomorrow morning work day start u...\n",
       "49672               bring dunkin iced coffee tomorrow hero\n",
       "49673    currently holiday portugal come home tomorrow ...\n",
       "49674                         ladykiller saturday aternoon\n",
       "Name: message, Length: 49675, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_tokenized_message = pd.Series(tweets_train_tokenized.message)\n",
    "tweets_train_tokenized_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['arirang simply kpop kim hyung jun cross ha yeong playback',\n",
       "       'read politico article donald trump running mate tom brady list likely choice',\n",
       "       'type bazura project google image image photo dad glenn moustache whatthe',\n",
       "       ..., 'bring dunkin iced coffee tomorrow hero',\n",
       "       'currently holiday portugal come home tomorrow poland tuesday holocaust memorial trip',\n",
       "       'ladykiller saturday aternoon'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Panda series into Unicode datatype as required by vectorizers\n",
    "tweets = tweets_train_tokenized_message.astype('U').values\n",
    "tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **II. Creating vectors from text**\n",
    "\n",
    "1. It should not result in a sparse matrix since sparse matrices result in high computation cost\n",
    "2. We should be able to retain most of the linguistic information present in the sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A. Bag of Words Model (addendum)**\n",
    "\n",
    "It’s the simplest model, and the idea is to take the whole text data and count their frequency of occurrence. and map the words with their frequency. This method doesn’t care about the order of the words, but it does care how many times a word occurs and the default bag of words model treats all words equally.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Sparsity is a problem, given there are many words in reality each sentence, each has to be converted to 0 or 1.\n",
    "2. Ordering of words is changed and is not captured, because our feature index is based on their frequency (The feature with the highest frequency is at the beginning)\n",
    "3. We are not retaining any information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    "4. Out of Vocabulary problem exists, if we have a new word not in our vocabulary coming from our test data, it will get removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "A. Simple application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/bag-of-words-1.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/bag-of-words-1.png\", width=700, height=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/bag-of-words-2.png\" width=\"700\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/bag-of-words-2.png\", width=700, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/bag-of-words-3.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/bag-of-words-3.png\", width=700, height=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Coded application with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49675x317456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 866104 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BagOfWords\n",
    "# ngram_range specify the n-grams and accepts a tuple ie. (1,2)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vector = CountVectorizer(binary=True, ngram_range= (1,2))\n",
    "count_matrix = vector.fit_transform(tweets)\n",
    "count_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **B. Term-frequency Inverse-document Frequency (TF-IDF)**\n",
    "\n",
    "The BOW model doesn’t give good results since it has a drawback. Assume that there is a particular word that is appearing in all the documents and it comes multiple times, eventually, it will have a higher frequency of occurrence and it will have a greater value that will cause a specific word to have more weightage in a sentence, that’s not good for our analysis.\n",
    "\n",
    "The idea of TF-IDF is to reflect the importance of a word to its document or sentence by normalizing the words which occur frequently in the collection of documents.\n",
    "\n",
    "**Term-Frequency (TF)**\n",
    "\n",
    "It is a measure of how frequently a term $t$, appears in a document, $d$ :\n",
    "\n",
    "$$tf_{t,d} = \\frac {n_{t,d}} {number\\ of\\ terms\\ in\\ a\\ document} $$\n",
    "\n",
    "It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. \n",
    "\n",
    "**Inverse Document Frequency (IDF)**\n",
    "\n",
    "It is a measure of how rare a word is in a document. If a word appears in almost every document it is not significant for the classification.\n",
    "\n",
    "$$ idf_{t} = ln(\\frac {number\\ of\\ documents} {number\\ of\\ documents\\ with\\ term\\ t }) $$\n",
    "\n",
    "If a word has appeared in all the documents, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "It evaluates how relevant is a word to its sentence in a collection of sentences or documents.\n",
    "\n",
    "$$ (TFIDF)_{t,d} = tf_{t,d} * idf_{t} $$\n",
    "\n",
    "Words with a higher score are more important, and those with a lower score are less important.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Simple and intuitive\n",
    "2. Word importance is captured\n",
    "3. It performs much better for machine learning models than simple Bag of Words.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Sparsity is still present, but much less than Bag of Words.\n",
    "2. Out of vocabulary problem is still not handled.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "A. Simple application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/TFIDF-1.png\" width=\"700\" height=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/TFIDF-1.png\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/TFIDF-2.png\" width=\"700\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/TFIDF-2.png\", width=700, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/TFIDF-3.png\" width=\"700\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/TFIDF-3.png\", width=700, height=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Coded Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49675x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 426709 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# you can also specify ngram_range\n",
    "# you can also choose the max_features parameter, which just includes those features with the top frequencies specified by the max_features i.e.\n",
    "# max_features = 3, includes only those top 3 features with the highest frequencies\n",
    "# ngram_range=(1,2)\n",
    "#tfidf = TfidfVectorizer(min_df =2, max_features=4000, ngram_range=(1,2))\n",
    "tfidf = TfidfVectorizer(min_df =2, max_features=5000, ngram_range=(1,2))\n",
    "#tfidf = TfidfVectorizer(max_features=4000, ngram_range=(1,2))\n",
    "tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "tfidf_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_y = pd.read_csv('csvs/tweets_train_y.csv', index_col=False)\n",
    "tweets_train_y = pd.Series(tweets_train_y['0'])\n",
    "tweets_train_y = tweets_train_y.values\n",
    "tweets_train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_codes = {'negative': 0, 'neutral': 1, 'positive': 2}\n",
    "\n",
    "f1_pos_neg = metrics.make_scorer(\n",
    "    metrics.f1_score, average=\"macro\", labels=[labels_codes[\"negative\"], labels_codes[\"positive\"]]\n",
    ")\n",
    "# micro-recall\n",
    "#'micro' - Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "# we cannot use pos-label = 0 discussed in specificity here because we have a multi-class classification, not binary\n",
    "# label=[pos-label] will report scores for that label only\n",
    "recall_neg = metrics.make_scorer(metrics.recall_score, average=\"micro\", labels=[labels_codes[\"negative\"]])\n",
    "\n",
    "\n",
    "def evaluate_model(model, features, labels, cv=5, fit_params=None):\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        features,\n",
    "        labels,\n",
    "        cv=cv,\n",
    "        fit_params=fit_params,\n",
    "        scoring={\n",
    "            \"recall_macro\": \"recall_macro\",\n",
    "            \"f1_pos_neg\": f1_pos_neg,\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"recall_neg\": recall_neg,\n",
    "        },\n",
    "        n_jobs=-1,  # this means that each metric will be computed using all cores of your computer processing unit\n",
    "    )\n",
    "\n",
    "    results = pd.DataFrame(scores).drop([\"fit_time\", \"score_time\"], axis=1)\n",
    "    results.columns = pd.MultiIndex.from_tuples([c.split(\"_\", maxsplit=1) for c in results.columns])\n",
    "    summary = results.describe()\n",
    "    results = pd.concat([results, summary.loc[[\"mean\", \"std\"]]])\n",
    "\n",
    "    def custom_style(row):\n",
    "        color = \"white\"\n",
    "        if row.name == \"mean\":\n",
    "            color = \"orange\"\n",
    "        return [\"background-color: %s\" % color] * len(row.values)\n",
    "\n",
    "    results = results[sorted(results.columns, key=lambda x: x[0], reverse=True)]\n",
    "    results = results.style.apply(custom_style, axis=1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", class_weight=\"balanced\", max_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c567d_row0_col0, #T_c567d_row0_col1, #T_c567d_row0_col2, #T_c567d_row0_col3, #T_c567d_row1_col0, #T_c567d_row1_col1, #T_c567d_row1_col2, #T_c567d_row1_col3, #T_c567d_row2_col0, #T_c567d_row2_col1, #T_c567d_row2_col2, #T_c567d_row2_col3, #T_c567d_row3_col0, #T_c567d_row3_col1, #T_c567d_row3_col2, #T_c567d_row3_col3, #T_c567d_row4_col0, #T_c567d_row4_col1, #T_c567d_row4_col2, #T_c567d_row4_col3, #T_c567d_row6_col0, #T_c567d_row6_col1, #T_c567d_row6_col2, #T_c567d_row6_col3 {\n",
       "  background-color: white;\n",
       "}\n",
       "#T_c567d_row5_col0, #T_c567d_row5_col1, #T_c567d_row5_col2, #T_c567d_row5_col3 {\n",
       "  background-color: orange;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c567d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c567d_level0_col0\" class=\"col_heading level0 col0\" colspan=\"4\">test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"blank level1\" >&nbsp;</th>\n",
       "      <th id=\"T_c567d_level1_col0\" class=\"col_heading level1 col0\" >recall_macro</th>\n",
       "      <th id=\"T_c567d_level1_col1\" class=\"col_heading level1 col1\" >f1_pos_neg</th>\n",
       "      <th id=\"T_c567d_level1_col2\" class=\"col_heading level1 col2\" >accuracy</th>\n",
       "      <th id=\"T_c567d_level1_col3\" class=\"col_heading level1 col3\" >recall_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c567d_row0_col0\" class=\"data row0 col0\" >0.626513</td>\n",
       "      <td id=\"T_c567d_row0_col1\" class=\"data row0 col1\" >0.596663</td>\n",
       "      <td id=\"T_c567d_row0_col2\" class=\"data row0 col2\" >0.622446</td>\n",
       "      <td id=\"T_c567d_row0_col3\" class=\"data row0 col3\" >0.635249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c567d_row1_col0\" class=\"data row1 col0\" >0.635832</td>\n",
       "      <td id=\"T_c567d_row1_col1\" class=\"data row1 col1\" >0.606322</td>\n",
       "      <td id=\"T_c567d_row1_col2\" class=\"data row1 col2\" >0.626472</td>\n",
       "      <td id=\"T_c567d_row1_col3\" class=\"data row1 col3\" >0.663871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c567d_row2_col0\" class=\"data row2 col0\" >0.632532</td>\n",
       "      <td id=\"T_c567d_row2_col1\" class=\"data row2 col1\" >0.600897</td>\n",
       "      <td id=\"T_c567d_row2_col2\" class=\"data row2 col2\" >0.630700</td>\n",
       "      <td id=\"T_c567d_row2_col3\" class=\"data row2 col3\" >0.634839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c567d_row3_col0\" class=\"data row3 col0\" >0.632406</td>\n",
       "      <td id=\"T_c567d_row3_col1\" class=\"data row3 col1\" >0.601818</td>\n",
       "      <td id=\"T_c567d_row3_col2\" class=\"data row3 col2\" >0.632209</td>\n",
       "      <td id=\"T_c567d_row3_col3\" class=\"data row3 col3\" >0.629677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c567d_row4_col0\" class=\"data row4 col0\" >0.632781</td>\n",
       "      <td id=\"T_c567d_row4_col1\" class=\"data row4 col1\" >0.602636</td>\n",
       "      <td id=\"T_c567d_row4_col2\" class=\"data row4 col2\" >0.630398</td>\n",
       "      <td id=\"T_c567d_row4_col3\" class=\"data row4 col3\" >0.635249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row5\" class=\"row_heading level0 row5\" >mean</th>\n",
       "      <td id=\"T_c567d_row5_col0\" class=\"data row5 col0\" >0.632013</td>\n",
       "      <td id=\"T_c567d_row5_col1\" class=\"data row5 col1\" >0.601667</td>\n",
       "      <td id=\"T_c567d_row5_col2\" class=\"data row5 col2\" >0.628445</td>\n",
       "      <td id=\"T_c567d_row5_col3\" class=\"data row5 col3\" >0.639777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c567d_level0_row6\" class=\"row_heading level0 row6\" >std</th>\n",
       "      <td id=\"T_c567d_row6_col0\" class=\"data row6 col0\" >0.003385</td>\n",
       "      <td id=\"T_c567d_row6_col1\" class=\"data row6 col1\" >0.003474</td>\n",
       "      <td id=\"T_c567d_row6_col2\" class=\"data row6 col2\" >0.003967</td>\n",
       "      <td id=\"T_c567d_row6_col3\" class=\"data row6 col3\" >0.013674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2533095ddc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(log_reg, tfidf_tweets, tweets_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectors/tfidf.sav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tfidvectorizer to disk\n",
    "tfidf_file = 'vectors/tfidf.sav'\n",
    "joblib.dump(tfidf, tfidf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectors/tfidf_tweets.sav']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tfidvectorizer to disk\n",
    "tfidf_tweets_file = 'vectors/tfidf_tweets.sav'\n",
    "joblib.dump(tfidf_tweets, tfidf_tweets_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **End. Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcad393ebe0ddd96229d28636729608eefe2a539d1a2b16c2babaf1f7828873b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
