{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the Tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arirang simply kpop kim hyung jun cross ha yeo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>read politico article donald trump running mat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>type bazura project google image image photo d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fast lerner subpoena tech guy work hillary pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sony reward app like lot female singer non ret...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message\n",
       "0  arirang simply kpop kim hyung jun cross ha yeo...\n",
       "1  read politico article donald trump running mat...\n",
       "2  type bazura project google image image photo d...\n",
       "3  fast lerner subpoena tech guy work hillary pri...\n",
       "4  sony reward app like lot female singer non ret..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_tokenized = pd.read_csv('csvs/tweets_train_tokens.csv', index_col=False)\n",
    "tweets_train_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        arirang simply kpop kim hyung jun cross ha yeo...\n",
       "1        read politico article donald trump running mat...\n",
       "2        type bazura project google image image photo d...\n",
       "3        fast lerner subpoena tech guy work hillary pri...\n",
       "4        sony reward app like lot female singer non ret...\n",
       "                               ...                        \n",
       "49670    sleep think fuck jordan answer phone tomorrow ...\n",
       "49671    yoga shannon tomorrow morning work day start u...\n",
       "49672               bring dunkin iced coffee tomorrow hero\n",
       "49673    currently holiday portugal come home tomorrow ...\n",
       "49674                         ladykiller saturday aternoon\n",
       "Name: message, Length: 49675, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train_tokenized_message = pd.Series(tweets_train_tokenized.message)\n",
    "tweets_train_tokenized_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['arirang simply kpop kim hyung jun cross ha yeong playback',\n",
       "       'read politico article donald trump running mate tom brady list likely choice',\n",
       "       'type bazura project google image image photo dad glenn moustache whatthe',\n",
       "       ..., 'bring dunkin iced coffee tomorrow hero',\n",
       "       'currently holiday portugal come home tomorrow poland tuesday holocaust memorial trip',\n",
       "       'ladykiller saturday aternoon'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Panda series into Unicode datatype as required by vectorizers\n",
    "tweets = tweets_train_tokenized_message.astype('U').values\n",
    "tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **II. Creating vectors from text**\n",
    "\n",
    "1. It should not result in a sparse matrix since sparse matrices result in high computation cost\n",
    "2. We should be able to retain most of the linguistic information present in the sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A. Bag of Words Model (addendum)**\n",
    "\n",
    "It’s the simplest model, and the idea is to take the whole text data and count their frequency of occurrence. and map the words with their frequency. This method doesn’t care about the order of the words, but it does care how many times a word occurs and the default bag of words model treats all words equally.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Sparsity is a problem, given there are many words in reality each sentence, each has to be converted to 0 or 1.\n",
    "2. Ordering of words is changed and is not captured, because our feature index is based on their frequency (The feature with the highest frequency is at the beginning)\n",
    "3. We are not retaining any information on the grammar of the sentences nor on the ordering of the words in the text.\n",
    "4. Out of Vocabulary problem exists, if we have a new word not in our vocabulary coming from our test data, it will get removed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "A. Simple application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/bag-of-words-1.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/bag-of-words-1.png\", width=700, height=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/bag-of-words-2.png\" width=\"700\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/bag-of-words-2.png\", width=700, height=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/bag-of-words-3.png\" width=\"700\" height=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/bag-of-words-3.png\", width=700, height=500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Coded application with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49675x317456 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 866104 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BagOfWords\n",
    "# ngram_range specify the n-grams and accepts a tuple ie. (1,2)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vector = CountVectorizer(binary=True, ngram_range= (1,2))\n",
    "count_matrix = vector.fit_transform(tweets)\n",
    "count_matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **B. Term-frequency Inverse-document Frequency (TF-IDF)**\n",
    "\n",
    "The BOW model doesn’t give good results since it has a drawback. Assume that there is a particular word that is appearing in all the documents and it comes multiple times, eventually, it will have a higher frequency of occurrence and it will have a greater value that will cause a specific word to have more weightage in a sentence, that’s not good for our analysis.\n",
    "\n",
    "The idea of TF-IDF is to reflect the importance of a word to its document or sentence by normalizing the words which occur frequently in the collection of documents.\n",
    "\n",
    "**Term-Frequency (TF)**\n",
    "\n",
    "It is a measure of how frequently a term $t$, appears in a document, $d$ :\n",
    "\n",
    "$$tf_{t,d} = \\frac {n_{t,d}} {number\\ of\\ terms\\ in\\ a\\ document} $$\n",
    "\n",
    "It denotes the contribution of the word to the document i.e words relevant to the document should be frequent. \n",
    "\n",
    "**Inverse Document Frequency (IDF)**\n",
    "\n",
    "It is a measure of how rare a word is in a document. If a word appears in almost every document it is not significant for the classification.\n",
    "\n",
    "$$ idf_{t} = ln(\\frac {number\\ of\\ documents} {number\\ of\\ documents\\ with\\ term\\ t }) $$\n",
    "\n",
    "If a word has appeared in all the documents, then probably that word is not relevant to a particular document. But if it has appeared in a subset of documents then probably the word is of some relevance to the documents it is present in.\n",
    "\n",
    "**TF-IDF**\n",
    "\n",
    "It evaluates how relevant is a word to its sentence in a collection of sentences or documents.\n",
    "\n",
    "$$ (TFIDF)_{t,d} = tf_{t,d} * idf_{t} $$\n",
    "\n",
    "Words with a higher score are more important, and those with a lower score are less important.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "1. Simple and intuitive\n",
    "2. Word importance is captured\n",
    "3. It performs much better for machine learning models than simple Bag of Words.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1. Sparsity is still present, but much less than Bag of Words.\n",
    "2. Out of vocabulary problem is still not handled.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "A. Simple application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/TFIDF-1.png\" width=\"700\" height=\"700\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/TFIDF-1.png\", width=700, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/TFIDF-2.png\" width=\"700\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/TFIDF-2.png\", width=700, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"pictures/TFIDF-3.png\" width=\"700\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "\n",
    "# get the image\n",
    "Image(url=\"pictures/TFIDF-3.png\", width=700, height=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Coded Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<49675x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 426709 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# you can also specify ngram_range\n",
    "# you can also choose the max_features parameter, which just includes those features with the top frequencies specified by the max_features i.e.\n",
    "# max_features = 3, includes only those top 3 features with the highest frequencies\n",
    "# ngram_range=(1,2)\n",
    "#tfidf = TfidfVectorizer(min_df =2, max_features=4000, ngram_range=(1,2))\n",
    "tfidf = TfidfVectorizer(min_df =2, max_features=5000, ngram_range=(1,2))\n",
    "#tfidf = TfidfVectorizer(max_features=4000, ngram_range=(1,2))\n",
    "tfidf_tweets = tfidf.fit_transform(tweets)\n",
    "tfidf_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectors/tfidf.sav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tfidvectorizer to disk\n",
    "tfidf_file = 'vectors/tfidf.sav'\n",
    "joblib.dump(tfidf, tfidf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vectors/tfidf_tweets.sav']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the tfidvectorizer to disk\n",
    "tfidf_tweets_file = 'vectors/tfidf_tweets.sav'\n",
    "joblib.dump(tfidf_tweets, tfidf_tweets_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **End. Thank you!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcad393ebe0ddd96229d28636729608eefe2a539d1a2b16c2babaf1f7828873b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
